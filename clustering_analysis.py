Problem StatementScaler is an online tech-versity offering intensive computer science & Data Science courses through live classes delivered by tech leaders and subject matter experts.The meticulously structured program enhances the skills of software professionals by offering a modern curriculum with exposure to the latest technologies. It is a product by InterviewBit.You are working as a data scientist with the analytics vertical of Scaler, focused on profiling the best companies and job positions to work for from the Scaler database.You are provided with the information for a segment of learners and tasked to cluster them on the basis of their job profile, company, and other features. Ideally, these clusters should have similar characteristics.Data Dictionary:‘Unnamed 0’- Index of the datasetEmail_hash- Anonymised Personal Identifiable Information (PII)Company_hash- Current employer of the learnerorgyear- Employment start dateCTC- Current CTCJob_position- Job profile in the companyCTC_updated_year: Year in which CTC got updated (Yearly increments, Promotions)Concept Used:Manual ClusteringUnsupervised Clustering - K- means, Hierarchical Clustering
import re
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
plt.rcParams["figure.figsize"] = (12,8)
import warnings
warnings.filterwarnings("ignore")
df = pd.read_csv("scaler_clustering.csv",index_col=0)
# Output: company_hashemail_hashorgyearctcjob_positionctc_updated_year17621bvi ogenfvqte93abc6cafbd171f08953540ecf510f10dd3c29698fe2d...2015.0200000Frontend Engineer2021.043264qfo95359fcf297402a0fd09a5d467e90647494e1820fb4091...2018.0600000NaN2021.06416zgztfb69e1bf6d85b39e4759ad3db8a1a55c1175c240108cca...2016.0450000Devops Engineer2020.035431fyttrotjt ntwyzgrgsj21f6b7f3bd41a215b0fff15baf9a2253a8eba2fd0127b7...2018.0200000NaN2021.0109059xzegojo630b0d4ce7833b3a0f4985f36ea19b76c483523be204b6...2020.0525000FullStack Engineer2021.02417qxv vacxogqjadf6018a5bdfcd819beb86808e9c3ed2ea954a543f7dbf...2020.0700000NaN2021.0175594sggsrt8e4b39577f3b328db8ef87cbc841a9fa18be0983157416...2018.01950000Frontend Engineer2020.047541hztburgjtab4a2b543479e569cbb4591e4490f7685b0856540c08094...2018.024000NaN2020.084285ovu08a1ffc2306b7b84edb7081c030c34df39858269cdcd2a...2015.0930000Frontend Engineer2018.0134205gzbgmxrt srgmvr rxbxnta491c9b3c8df401e916538f4d9d39c8a3fee1a39d7db834...2012.01700000FullStack Engineer2018.0# Output: (205843, 6)<class 'pandas.core.frame.DataFrame'>Int64Index: 205843 entries, 0 to 206922Data columns (total 6 columns):#   Column            Non-Null Count   Dtype  ---  ------            --------------   -----  0   company_hash      205799 non-null  object 1   email_hash        205843 non-null  object 2   orgyear           205757 non-null  float643   ctc               205843 non-null  int64  4   job_position      153281 non-null  object 5   ctc_updated_year  205843 non-null  float64dtypes: float64(2), int64(1), object(3)memory usage: 11.0+ MB# Output: company_hash           44email_hash              0orgyear                86ctc                     0job_position        52562ctc_updated_year        0dtype: int64
df.sample(10)
df.shape# 205843 learners data
df.info()
df.isna().sum()
# Output: company_hash         0.021376email_hash           0.000000orgyear              0.041779ctc                  0.000000job_position        25.534995ctc_updated_year     0.000000dtype: float64# Output: orgyearctcctc_updated_yearcount205757.0000002.058430e+05205843.000000mean2014.8827502.271685e+062019.628231std63.5711151.180091e+071.325104min0.0000002.000000e+002015.00000025%2013.0000005.300000e+052019.00000050%2016.0000009.500000e+052020.00000075%2018.0000001.700000e+062021.000000max20165.0000001.000150e+092021.000000
# Output: company_hashemail_hashjob_positioncount205799205843153281unique372991534431017topnvnv wgzohrnvzwj otqcxwtobbace3cc586400bbc65765bc6a16b77d8913836cfc98b7...Backend Engineerfreq83371043554
# Output: 'airtel x labs'(
df.isna().sum()/ len(df))*100
df.describe()
# based on above information , noticing some unusual outliers in the data   df.describe(include="object")
def preprocess_string(string):    new_string= re.sub('[^A-Za-z ]+', '', string).lower().strip()    return new_string    mystring='\tAirtel\\\\&&**() X Labs'preprocess_string(mystring)
# Output: 37299# Output: 37208
# Output: 1017# Output: 857
# Output: company_hashorgyearctcjob_positionctc_updated_year135202nxmwg ogenfvqt xzw2014.0270000backend engineer2016.08043st2012.01320000backend engineer2019.0191058vbvkgz rvm2010.0220000fullstack engineer2019.0190577vagmt2016.02200000devops engineer2019.064059obvqnqgz2014.0650000android engineer2019.0# Output: 17597df["company_hash"].nunique()df["company_hash"] = df["company_hash"].apply(lambda x: preprocess_string(str(x))) df["company_hash"].nunique() df["job_position"].nunique()# 1017 unique job positions are there in the dataset df["job_position"] = df["job_position"].apply(lambda x: preprocess_string(str(x))) df["job_position"].nunique() # 857 unique job positions are there in the dataset after preprocessing strings  # removing the email_hashdf.drop("email_hash",axis = 1,inplace=True)
df.sample(5)
df.duplicated().sum() # 17597 duplicated records
# Output: company_hash         0orgyear             86ctc                  0job_position         0ctc_updated_year     0dtype: int64# Output: 89# Output: 44# Output: 9# Output: 52562# Output: company_hashorgyearctcjob_positionctc_updated_year1677172018.01500000backend engineer2020.0769072021.0800000nan2021.0253332019.02000000nan2021.02021792016.0500000nan2017.0841922018.01400000backend engineer2019.01979782020.01000000nan2019.0504142020.0720000nan2019.01175712010.04500000nan2019.01276792019.01400000backend engineer2019.0806682019.0850000nan2019.0# Output: 98
df.isna().sum()
(df["company_hash"] == "").sum()(df["company_hash"] == "nan").sum()(df["job_position"] == "").sum()(df["job_position"] == "nan").sum()# removing the records where company or job_position reocords are not available df[(df["company_hash"] == "") | (df["job_position"] == "")].sample(10)
len(df[(df["company_hash"] == "") | (df["job_position"] == "")])# df[((df["company_hash"] != "") & (df["job_position"] != ""))]
# Data PreprocessingIn [156]:imputing Employee Start Year as per the median year as per each company.In [157]:
# Output: company_hashorgyearctcjob_positionctc_updated_year0atrgxnnt xzaxv2016.01100000other2020.01qtrxvzwt xzegwgbb rxbxnta2018.0449999fullstack engineer2019.02ojzwnvwnxw vx2015.02000000backend engineer2020.03ngpgutaxv2017.0700000backend engineer2019.04qxen sqghu2017.01400000fullstack engineer2019.0..................206918vuurt xzw2008.0220000nan2019.0206919husqvawgb2017.0500000nan2020.0206920vwwgrxnt2021.0700000nan2021.0206921zgn vuurxwvmrt2019.05100000nan2019.0206922bgqsvz onvzrtj2014.01240000nan2016.0205745 rows × 5 columns
# Output: 86# Output: 0         2014.01         2016.02         2015.03         2016.04         2017.0          ...  206918    2018.0206919    2017.0206920    2016.0206921    2020.0206922    2015.0Name: orgyear, Length: 205745, dtype: float64# Output: 0df = df[~((df["company_hash"] == "") | (df["job_position"] == ""))]df
df["orgyear"].isna().sum()df.groupby("company_hash")["orgyear"].transform("median")
df["orgyear"].fillna(df['orgyear'].isnull().sum(),inplace=True)df["orgyear"].isna().sum()
Outliers Treatment :employement start year# Output: company_hashorgyearctcjob_positionctc_updated_year175831bxqtrk2013.02500000fullstack engineer2019.049006wxnx2018.01500000backend engineer2021.021098tdr2015.0730000other2020.0151312nvnv wgzohrnvzwj otqcxwto2020.0700000fullstack engineer2020.0153058vwwtznhqt2016.0700000nan2021.0
# Output: 2018.0    252402019.0    234022017.0    232372016.0    230382015.0    20602         ...  2107.0        11972.0        12101.0        1208.0         1200.0         1Name: orgyear, Length: 78, dtype: int64
df.sample(5)
df["orgyear"].value_counts()
<IPython.core.display.Javascript object>
# Output: 1990.0# Output: 2023.0sns.countplot(df["orgyear"])plt.xticks(rotation = 90)plt.show()
# sns.histplot(np.log(df["orgyear"]))df["orgyear"].quantile(0.001)df["orgyear"].quantile(0.999)df["orgyear"] = df["orgyear"].clip(1990,2022)
ctc updated_year<IPython.core.display.Javascript object>
# Output: 2015.0# Output: 2021.0sns.countplot(df["orgyear"])plt.xticks(rotation = 90)plt.show()
df["ctc_updated_year"].quantile(0.001)df["ctc_updated_year"].quantile(0.99)
outlier treatment for CTC<IPython.core.display.Javascript object>
# Output: 37000.0# Output: 200000000.0sns.countplot(df["ctc_updated_year"])plt.xticks(rotation = 90)plt.show()
df["ctc"].quantile(0.01)df["ctc"].quantile(0.999)df = df.loc[((df.ctc) > df.ctc.quantile(0.01)) & ((df.ctc) < df.ctc.quantile(0.99))]
replacing string "nan" to np.nan# Output: company_hashorgyearctcjob_positionctc_updated_year0atrgxnnt xzaxv2016.01100000other2020.01qtrxvzwt xzegwgbb rxbxnta2018.0449999fullstack engineer2019.02ojzwnvwnxw vx2015.02000000backend engineer2020.03ngpgutaxv2017.0700000backend engineer2019.04qxen sqghu2017.01400000fullstack engineer2019.0..................206918vuurt xzw2008.0220000nan2019.0206919husqvawgb2017.0500000nan2020.0206920vwwgrxnt2021.0700000nan2021.0206921zgn vuurxwvmrt2019.05100000nan2019.0206922bgqsvz onvzrtj2014.01240000nan2016.0201625 rows × 5 columns<IPython.core.display.Javascript object># Output: <AxesSubplot:xlabel='ctc', ylabel='Density'>
df
# sns.distplot(df["ctc"])
Feature EngineeringMasked company name to "Others" having count less than 5
years of experience = current year - employement start year# Output: 46434
# Output: count    201625.000000mean       2015.104769std           4.256063min        1990.00000025%        2013.00000050%        2016.00000075%        2018.000000max        2022.000000Name: orgyear, dtype: float64df.loc[df['job_position']=='nan', 'job_position']=np.nandf.loc[df["company_hash"]=="nan","company_hash"] = np.nan    # df.company_hash.value_counts(dropna=False)# df.job_position.value_counts(dropna=False)
df.loc[df.groupby("company_hash")["ctc"].transform("count") < 5,"company_hash"] = "Others"(df["company_hash"] == "Others").sum()# df.company_hash.value_counts(dropna=False) df['orgyear'].describe()
# years of experience df["years_of_experience_in_organization"] = 2023 - df["orgyear"]
# Output: company_hashorgyearctcjob_positionctc_updated_yearyears_of_experience_in_organization157723Others2019.0480999NaN2018.04.085654xzegojo2018.0900000other2020.05.0<IPython.core.display.Javascript object># Output: <AxesSubplot:xlabel='years_of_experience_in_organization', ylabel='count'>
# Output: 37683# Output: (163942, 6)
df.sample(2)sns.countplot(df["years_of_experience_in_organization"])
df.duplicated().sum() df.drop_duplicates(inplace=True)
df.shape
# treating records having ctc_updated_year higher than their organization joining yearIn [190]:In [191]:In [192]:In [193]:
# Filling null values with others -- if not done beforeIn [194]:In [ ]:In [195]:
# Output: company_hash                              42orgyear                                    0ctc                                        0job_position                           36745ctc_updated_year                           0years_of_experience_in_organization        0dtype: int64
# Output: 7181# Output: 0# Output: company_hashorgyearctcjob_positionctc_updated_yearyears_of_experience_in_organization198803bgqsvz onvzrtj2017.01600000NaN2019.06.0178348bjznqvlvmgzs2017.01970000NaN2017.06.0
# Output: company_hash                           0orgyear                                0ctc                                    0job_position                           0ctc_updated_year                       0years_of_experience_in_organization    0dtype: int64
df.isna().sum()
# records having ctc_updated_year higher than their organization joining year(df["ctc_updated_year"] < df["orgyear"]).sum()df.ctc_updated_year = df[["ctc_updated_year","orgyear"]].max(axis = 1)(df["ctc_updated_year"] < df["orgyear"]).sum()
df.sample(2)
df['job_position'] = df['job_position'].fillna('Others')df['company_hash'] = df['company_hash'].fillna('Others')
df.isna().sum()
# Output: 1061
# Output: orgyearctcctc_updated_yearyears_of_experience_in_organizationcount163942.0000001.639420e+05163942.000000163942.000000mean2014.7722181.425498e+062019.5955408.227782std4.4020531.303985e+061.3349624.402053min1990.0000003.800000e+042015.0000001.00000025%2013.0000006.000000e+052019.0000005.00000050%2016.0000001.039999e+062020.0000007.00000075%2018.0000001.800000e+062021.00000010.000000max2022.0000001.250000e+072022.00000033.000000<class 'pandas.core.frame.DataFrame'>Int64Index: 163942 entries, 0 to 206922Data columns (total 6 columns):#   Column                               Non-Null Count   Dtype  ---  ------                               --------------   -----  0   company_hash                         163942 non-null  object 1   orgyear                              163942 non-null  float642   ctc                                  163942 non-null  int64  3   job_position                         163942 non-null  object 4   ctc_updated_year                     163942 non-null  float645   years_of_experience_in_organization  163942 non-null  float64dtypes: float64(3), int64(1), object(2)memory usage: 8.8+ MB df.duplicated().sum() # df.drop_duplicates(inplace=True) # glacing over data after outlier treatment and preprocessing
df.describe()
df.info()
Manual Clustering based on Company , Job position and Years of experienceLearner's "designation_in_organization"<IPython.core.display.Javascript object># Output: <AxesSubplot:xlabel='ctc', ylabel='years_of_experience_in_organization'>
# Output: Index(['company_hash', 'orgyear', 'ctc', 'job_position', 'ctc_updated_year',      'years_of_experience_in_organization'],     dtype='object')sns.scatterplot(df.ctc,df.years_of_experience_in_organization)
df.columns
# Output: countmeanstdmin25%50%75%max years_of_experience_in_organizationjob_positioncompany_hash1.0OthersOthers58.01.586207e+062.080212e+0660000.0407500.0750000.01575000.010000000.0agzn fgqp xz vzj gqsvzxkvnxgz1.01.600000e+06NaN1600000.01600000.01600000.01600000.01600000.0atrgxnnt1.01.000000e+06NaN1000000.01000000.01000000.01000000.01000000.0atrr1.01.000000e+06NaN1000000.01000000.01000000.01000000.01000000.0atrr ntwyzgrgsxto2.01.000000e+062.828427e+05800000.0900000.01000000.01100000.01200000.0.................................33.0qa engineerhzxntaytvrny sqghu1.05.400000e+05NaN540000.0540000.0540000.0540000.0540000.0tmxd ogenfvqt xzaxv ucn rna1.01.220000e+06NaN1220000.01220000.01220000.01220000.01220000.0utrvnqg ogrhnxgzo ucnrna1.06.000000e+05NaN600000.0600000.0600000.0600000.0600000.0research engineersovbohzs qa xzonxnhnt xzaxv atryx1.01.400000e+06NaN1400000.01400000.01400000.01400000.01400000.0support engineerOthers2.03.700000e+053.252691e+05140000.0255000.0370000.0485000.0600000.066191 rows × 8 columnsGROUPED_CTC = df.groupby(["years_of_experience_in_organization",                                                         "job_position",                                                         "company_hash"])["ctc"].describe()  GROUPED_CTC
df_GROUPED_CTC_BY_E_P_C = df.merge(GROUPED_CTC,        on = ["years_of_experience_in_organization",              "job_position",              "company_hash"],        how = "left")
# whichever learner has ctc compared to their years of experience , respective company , positiongiving designation as 3 when ctc is < 50th percentile in his position ,experience and companygiving designation as 2 when ctc is between 50th and 75th percentile in his position ,experience and companygiving designation as 1 when ctc is > 75th percentile in his position ,experience and companyIn [207]:Out[205]:company_hashorgyearctcjob_positionctc_updated_yearyears_of_experience_in_organizationcountmeanstdmin25%50%75%max 0atrgxnnt xzaxv2016.01100000other2020.07.01.01.100000e+06NaN1100000.01100000.01100000.01100000.01100000.0 1qtrxvzwt xzegwgbb rxbxnta2018.0449999fullstack engineer2019.05.07.07.742856e+052.509223e+05449999.0610000.0750000.0900000.01200000.0 2Others2015.02000000backend engineer2020.08.0440.01.269393e+061.405136e+0641000.0400000.0900000.01600000.010000000.0 3ngpgutaxv2017.0700000backend engineer2019.06.07.01.158571e+064.047810e+05700000.0825000.01200000.01405000.01750000.0 4qxen sqghu2017.01400000fullstack engineer2019.06.01.01.400000e+06NaN1400000.01400000.01400000.01400000.01400000.0 ............................................. 163937vuurt xzw2008.0220000Others2019.015.01.02.200000e+05NaN220000.0220000.0220000.0220000.0220000.0 163938husqvawgb2017.0500000Others2020.06.03.01.150000e+065.634714e+05500000.0975000.01450000.01475000.01500000.0 163939vwwgrxnt2021.0700000Others2021.02.03.06.666667e+053.511885e+05300000.0500000.0700000.0850000.01000000.0 163940zgn vuurxwvmrt2019.05100000Others2019.04.0118.01.412015e+061.715935e+0645000.0400000.0735000.01725250.010000000.0 163941bgqsvz onvzrtj2014.01240000Others2016.09.09.01.693333e+063.484250e+051200000.01500000.01700000.01900000.02200000.0 163942 rows × 14 columnsdf_GROUPED_CTC_BY_E_P_C
def classification(x,ctc_50,ctc_75):    if x < ctc_50:        return 3    elif x >= ctc_50 and x <= ctc_75:        return 2    elif  x >= ctc_75:        return 1
df_GROUPED_CTC_BY_E_P_C["designation_in_organization"] = df_GROUPED_CTC_BY_E_P_C.apply(lambda x:classification(x["ctc"],x["50%"],x["75%"]),axis = 1)
# Output: company_hashorgyearctcjob_positionctc_updated_yearyears_of_experience_in_organizationcountmeanstdmin25%50%75%maxdesignation_in_organization 0atrgxnnt xzaxv2016.01100000other2020.07.01.01.100000e+06NaN1100000.01100000.01100000.01100000.01100000.02 1qtrxvzwtxzegwgbbrxbxnta2018.0449999fullstackengineer2019.05.07.07.742856e+052.509223e+05449999.0610000.0750000.0900000.01200000.032Others2015.02000000backendengineer2020.08.0440.01.269393e+061.405136e+0641000.0400000.0900000.01600000.010000000.01 3ngpgutaxv2017.0700000backendengineer2019.06.07.01.158571e+064.047810e+05700000.0825000.01200000.01405000.01750000.03 4qxen sqghu2017.01400000fullstackengineer2019.06.01.01.400000e+06NaN1400000.01400000.01400000.01400000.01400000.02 ................................................ 163937vuurt xzw2008.0220000Others2019.015.01.02.200000e+05NaN220000.0220000.0220000.0220000.0220000.02 163938husqvawgb2017.0500000Others2020.06.03.01.150000e+065.634714e+05500000.0975000.01450000.01475000.01500000.03 163939vwwgrxnt2021.0700000Others2021.02.03.06.666667e+053.511885e+05300000.0500000.0700000.0850000.01000000.02 163940zgn vuurxwvmrt2019.05100000Others2019.04.0118.01.412015e+061.715935e+0645000.0400000.0735000.01725250.010000000.01 163941bgqsvz onvzrtj2014.01240000Others2016.09.09.01.693333e+063.484250e+051200000.01500000.01700000.01900000.02200000.03 163942 rows × 15 columns# Output: 2    0.4563933    0.3316601    0.211947Name: designation_in_organization, dtype: float64df_GROUPED_CTC_BY_E_P_C
df_GROUPED_CTC_BY_E_P_C.designation_in_organization.value_counts(normalize=True)
# Output: company_hashorgyearctcjob_positionctc_updated_yearyears_of_experience_in_organizationcountmeanstdmin25%50%75%maxdesignation_in_organization 0atrgxnnt xzaxv2016.01100000other2020.07.01.01.100000e+06NaN1100000.01100000.01100000.01100000.01100000.02 1qtrxvzwtxzegwgbbrxbxnta2018.0449999fullstackengineer2019.05.07.07.742856e+052.509223e+05449999.0610000.0750000.0900000.01200000.032Others2015.02000000backendengineer2020.08.0440.01.269393e+061.405136e+0641000.0400000.0900000.01600000.010000000.01 3ngpgutaxv2017.0700000backendengineer2019.06.07.01.158571e+064.047810e+05700000.0825000.01200000.01405000.01750000.03 4qxen sqghu2017.01400000fullstackengineer2019.06.01.01.400000e+06NaN1400000.01400000.01400000.01400000.01400000.02 ................................................ 163937vuurt xzw2008.0220000Others2019.015.01.02.200000e+05NaN220000.0220000.0220000.0220000.0220000.02 163938husqvawgb2017.0500000Others2020.06.03.01.150000e+065.634714e+05500000.0975000.01450000.01475000.01500000.03 163939vwwgrxnt2021.0700000Others2021.02.03.06.666667e+053.511885e+05300000.0500000.0700000.0850000.01000000.02 163940zgn vuurxwvmrt2019.05100000Others2019.04.0118.01.412015e+061.715935e+0645000.0400000.0735000.01725250.010000000.01 163941bgqsvz onvzrtj2014.01240000Others2016.09.09.01.693333e+063.484250e+051200000.01500000.01700000.01900000.02200000.03 163942 rows × 15 columns
# Output: company_hashorgyearctcjob_positionctc_updated_yearyears_of_experience_in_organizationdesignation_in_organization0atrgxnnt xzaxv2016.01100000other2020.07.021qtrxvzwt xzegwgbb rxbxnta2018.0449999fullstack engineer2019.05.032Others2015.02000000backend engineer2020.08.013ngpgutaxv2017.0700000backend engineer2019.06.034qxen sqghu2017.01400000fullstack engineer2019.06.02........................163937vuurt xzw2008.0220000Others2019.015.02163938husqvawgb2017.0500000Others2020.06.03163939vwwgrxnt2021.0700000Others2021.02.02163940zgn vuurxwvmrt2019.05100000Others2019.04.01163941bgqsvz onvzrtj2014.01240000Others2016.09.03163942 rows × 7 columnsdf_GROUPED_CTC_BY_E_P_C
df_GROUPED_CTC_BY_E_P_C.drop(columns=['count',                                     'mean',                                    'std',                                    'min',                                    '25%',                                    '50%',                                    '75%',                                    'max'],axis = 1,inplace=True) df_GROUPED_CTC_BY_E_P_C
Manual Clustering on company and job positiongrouping by each job_position and company ,finding which class of job an individual have,based on his ctc compared to his job_position and respective company.
# Output: (163942, 7)
# Output: countmeanstdmin25%50%75%maxjob_positioncompany_hashOthersOthers3520.01.366188e+061.445330e+0640000.0409999.0900000.01842499.2512500000.0a ntwyzgrgsxto6.01.229167e+061.401465e+06350000.0518750.0587500.01162500.004000000.0aaqxctz avnv owxtzwto vzvrjnxwo ucn rna1.05.000000e+05NaN500000.0500000.0500000.0500000.00500000.0abwavnv ojontb1.07.000000e+05NaN700000.0700000.0700000.0700000.00700000.0adw ntwyzgrgsj69.08.502319e+051.036041e+0680000.0380000.0500000.01000000.008000000.0..............................wordpress developerOthers1.06.000000e+05NaN600000.0600000.0600000.0600000.00600000.0workerzgn vuurxwvmrt vwwghzn1.02.000000e+05NaN200000.0200000.0200000.0200000.00200000.0xOthers1.04.000000e+05NaN400000.0400000.0400000.0400000.00400000.0young professional iisgctqzbtzn ge xzaxv1.05.000000e+05NaN500000.0500000.0500000.0500000.00500000.0zomatokgbvng2.03.000000e+052.828427e+05100000.0200000.0300000.0400000.00500000.025593 rows × 8 columns
# Output: company_hashorgyearctcjob_positionctc_updated_yearyears_of_experience_in_organizationcountmeanstdmin25%50%75%max 126677zvz2019.03200000data scientist2019.04.039.01.211500e+067.584555e+0546500.0647500.01000000.01650000.03200000.0 93215nvnv wgzohrnvzwj otqcxwto2012.0850000ios engineer2019.011.019.06.852632e+053.520475e+0570000.0425000.0600000.0850000.01550000.0 29447wvustbxzx2013.0910000backend engineer2021.010.0247.08.295992e+054.891276e+0540000.0475000.0700000.01070000.03000000.0 41080znn avnv otqcxwto2019.0700000Others2019.04.062.01.142984e+061.738646e+06300000.0400000.0690000.01275000.010000000.0 76917Others2013.01100000other2021.010.02367.01.117373e+061.423744e+0638000.0350000.0700000.01326000.012000000.0df_GROUPED_CTC_BY_E_P_C.shape
GROUPED_C_J=df.groupby(['job_position','company_hash'])['ctc'].describe()GROUPED_C_J
df_GROUPED_C_J=df.merge(GROUPED_C_J, on=['job_position','company_hash'], how='left') df_GROUPED_C_J.sample(5)
# Output: company_hashorgyearctcjob_positionctc_updated_yearyears_of_experience_in_organizationcountmeanstdmin25%50%75%maxclasss 79071ohnytqrvza2017.0710000data scientist2019.06.02.06.155000e+067.700393e+06710000.03432500.06155000.08877500.011600000.03 122059Others2019.0130000fullstack engineer2020.04.03181.01.193104e+061.532888e+0640000.0350000.0775000.01450000.012000000.03 35726wvwnho wgbbhzxwvnxgzo2013.0400000Others2020.010.03.07.333333e+055.773503e+05400000.0400000.0400000.0900000.01400000.02 68461vau2015.0819999backend engineer2019.08.051.01.124500e+069.053526e+05105000.0690000.0820000.01325000.06000000.03 42240wgszxkvzn2015.0800000frontend engineer2021.08.0105.07.959143e+055.380693e+0565000.0450000.0600000.01000000.04298000.02# Output: 3    0.4353732    0.3201011    0.244526Name: classs, dtype: float64
# Output: company_hash                           atrgxnnt xzaxvorgyear                                        2016.0ctc                                           1100000job_position                                    otherctc_updated_year                               2020.0years_of_experience_in_organization               7.0designation_in_organization                         2Name: 0, dtype: object# Output: company_hash                           atrgxnnt xzaxvorgyear                                        2016.0ctc                                           1100000job_position                                    otherctc_updated_year                               2020.0years_of_experience_in_organization               7.0classs                                              1Name: 0, dtype: object# creating classes basis on the salary in their respective companydf_GROUPED_C_J['classs'] = df_GROUPED_C_J.apply(lambda x: classification(x['ctc'],x['50%'],x['75%']),axis=1) df_GROUPED_C_J.sample(5)
df_GROUPED_C_J.classs.value_counts(normalize=True)df_GROUPED_C_J.drop(columns=['count',                                     'mean',                                    'std',                                    'min',                                    '25%',                                    '50%',                                    '75%',                                    'max'],axis = 1,inplace=True) df_GROUPED_CTC_BY_E_P_C.iloc[0]
df_GROUPED_C_J.iloc[0]
Manual Clustering based on comapnybased on ctc per company , assigning company as tier 1 2 and 3 per each learners
# Output: company_hashorgyearctcjob_positionctc_updated_yearyears_of_experience_in_organizationdesignation_in_organizationclasss107145cgjrrv evoyxgzo ucn rna2018.0900000fullstack engineer2018.05.023100926ztw ntwyzgrgsxto xzaxv rna2019.0540000Others2021.04.02362274eoo2019.0500000backend engineer2021.04.02210909vqwtoxhb2019.01500000Others2020.04.03360338bxzanqtt2017.0488000support engineer2020.06.023# Output: (166228, 8)
# Output: company_hashorgyearctcjob_positionctc_updated_yearyears_of_experience_in_organizationcountmeanstdmin25%50%75%max 46717Others2012.0700000ios engineer2019.011.026256.01.311366e+061.436286e+0638000.0440000.0900000.01650000.012500000.0 93680vwwgrxnt2013.01900000backend engineer2018.010.0165.01.414836e+066.917496e+05200000.01000000.01300000.01500000.04800000.0 136812x vb v onhatzn2018.0600000other2018.05.049.01.206531e+061.115764e+06100000.0500000.0900000.01700000.06500000.0 111948nguuq2018.01600000Others2019.05.072.01.707083e+061.085002e+06350000.0900000.01560000.02070000.05000000.0 9856Others2016.01440000frontend engineer2019.07.026256.01.311366e+061.436286e+0638000.0440000.0900000.01650000.012500000.0 df_Grouped = df_GROUPED_CTC_BY_E_P_C.merge(df_GROUPED_C_J, on=['company_hash',                                                      'orgyear',                                                      'ctc',                                                      'job_position',                                                      'years_of_experience_in_organization',                                                      'ctc_updated_year'], how='left') df_Grouped.sample(5)
df_Grouped.shape
GROUPED_C = df.groupby(['company_hash'])['ctc'].describe()df_company = df.merge(GROUPED_C, on=['company_hash'], how='left') df_company.sample(5)
df_company['tier'] =df_company.apply(lambda x: classification(x['ctc'],x['50%'],x['75%']),axis=1) # df_company.sample(5)
# Output: 3    0.4773642    0.2829111    0.239725Name: tier, dtype: float64
# Output: company_hash                           atrgxnnt xzaxvorgyear                                        2016.0ctc                                           1100000job_position                                    otherctc_updated_year                               2020.0years_of_experience_in_organization               7.0tier                                                2Name: 0, dtype: object# Output: company_hash                           atrgxnnt xzaxvorgyear                                        2016.0ctc                                           1100000job_position                                    otherctc_updated_year                               2020.0years_of_experience_in_organization               7.0designation_in_organization                         2classs                                              1Name: 0, dtype: objectdf_company.tier.value_counts(normalize=True)   df_company.drop(['count','mean','std','min','25%','50%','75%','max'],               axis = 1,               inplace=True)df_company.iloc[0]
df_Grouped.iloc[0]
df_Grouped = df_Grouped.merge(df_company,                 on=['company_hash',                     'orgyear','ctc',                     'job_position',                     'years_of_experience_in_organization',                     'ctc_updated_year'                    ])
Final data for Model :# Output: company_hashorgyearctcjob_positionctc_updated_yearyears_of_experience_in_organizationdesignation_in_organizationclassstier0atrgxnnt xzaxv2016.01100000other2020.07.02121qtrxvzwt xzegwgbb rxbxnta2018.0449999fullstack engineer2019.05.03332Others2015.02000000backend engineer2020.08.01113ngpgutaxv2017.0700000backend engineer2019.06.03334qxen sqghu2017.01400000fullstack engineer2019.06.0211..............................171311vuurt xzw2008.0220000Others2019.015.0233171312husqvawgb2017.0500000Others2020.06.0333171313vwwgrxnt2021.0700000Others2021.02.0233171314zgn vuurxwvmrt2019.05100000Others2019.04.0111171315bgqsvz onvzrtj2014.01240000Others2016.09.0333171316 rows × 9 columns
# Output: (171316, 9)
# Output: orgyearctcctc_updated_yearyears_of_experience_in_organizationdesignation_in_organizationclassstier02016.011000002020.07.021212018.04499992019.05.033322015.020000002020.08.011132017.07000002019.06.033342017.014000002019.06.0211........................1713112008.02200002019.015.02331713122017.05000002020.06.03331713132021.07000002021.02.02331713142019.051000002019.04.01111713152014.012400002016.09.0333171316 rows × 7 columnsdf_Grouped
X = df_Grouped.copy() X.shapeX_data = X.drop(["company_hash","job_position"],axis  = 1)X_data
# Standardization:In [242]:In [ ]:In [243]:
hierarchical Custering :trying to get a high level idea about how many clusters we can from, by taking sample of 500 learners multiple times and forming hierarchy and visualising in dendrogram.# Output: orgyearctcctc_updated_yearyears_of_experience_in_organizationdesignation_in_organizationclassstier00.229439-0.2384300.298195-0.229439-0.175910-1.497105-0.30055610.680950-0.741765-0.452799-0.6809501.1964141.0017070.93365520.0036830.4584930.298195-0.003683-1.548235-1.497105-1.53476630.455194-0.548174-0.452799-0.4551941.1964141.0017070.93365540.455194-0.006122-0.452799-0.455194-0.175910-1.497105-1.534766........................171311-1.576605-0.919866-0.4527991.576605-0.1759101.0017070.9336551713120.455194-0.7030460.298195-0.4551941.1964141.0017070.9336551713131.358216-0.5481741.049190-1.358216-0.1759101.0017070.9336551713140.9067052.859008-0.452799-0.906705-1.548235-1.497105-1.534766171315-0.222072-0.130020-2.7057820.2220721.1964141.0017070.933655171316 rows × 7 columnsfrom sklearn.preprocessing
import StandardScaler scaler = StandardScaler()scaler.fit(X_data)X_sc = pd.DataFrame(scaler.transform(X_data), columns=X_data.columns, index=X_data.index) X_sc
import scipy.cluster.hierarchy as sch
import matplotlib.pyplot as plt sample = X_sc.sample(500)Z = sch.linkage(sample, method='ward') fig, ax1 = plt.subplots(figsize=(20, 12))sch.dendrogram(Z, labels=sample.index, ax=ax1, color_threshold=2)plt.xticks(rotation=90)ax1.set_ylabel('distance')plt.show()
import scipy.cluster.hierarchy as sch
import matplotlib.pyplot as plt sample = X_sc.sample(500)Z = sch.linkage(sample, method='ward') fig, ax2 = plt.subplots(figsize=(20, 12))sch.dendrogram(Z, labels=sample.index, ax=ax2, color_threshold=2)plt.xticks(rotation=90)ax2.set_ylabel('distance')plt.show()
import scipy.cluster.hierarchy as sch
import matplotlib.pyplot as plt sample = X_sc.sample(500)Z = sch.linkage(sample, method='ward') fig, ax3 = plt.subplots(figsize=(20, 12))sch.dendrogram(Z, labels=sample.index, ax=ax3, color_threshold=2)plt.xticks(rotation=90)ax3.set_ylabel('distance')plt.show()
# Based on dendrogram , we can observe there are 3 clusters in the data based on similarityFurther checking appropriate number of clusters using Elbow Method using k-Means clustering :In [ ]:
# KMeansIn [ ]:
# Output: [1199211.9999999972,812618.2236265242,663951.3689564556,577020.6292578052,517714.4060221886,476402.90178635635,439357.96141059144,410144.6171733509,383988.5907258121]for i in range(1,10):    from sklearn.cluster
import KMeans     k = 4     kM = KMeans(n_clusters=k,               random_state=654)    y_pred = kM.fit_predict(X_sc) kmeans_per_k = [KMeans(n_clusters=k, random_state=42).fit(X_sc)                for k in range(1, 10)] inertias = [model.inertia_ for model in kmeans_per_k]inertias
KMeans with n_clusters = 3
plt.figure(figsize=(12, 8))plt.plot(range(1, 10), inertias, "bo-")plt.xlabel("$k$", fontsize=14)plt.ylabel("Inertia", fontsize=14)plt.annotate('Elbow',             xy=(3, inertias[2]),             xytext=(0.55, 0.55),             textcoords='figure fraction',             fontsize=16,             arrowprops=dict(facecolor='black', shrink=0.1)            )plt.show()
from sklearn.cluster
import KMeans k = 3 kM = KMeans(n_clusters=k,           random_state=654)y_pred = kM.fit_predict(X_sc) clusters = pd.DataFrame(X, columns=X.columns)clusters['label'] = kM.labels_
Insights | EDA after Clustering :# Output: company_hashorgyearctcjob_positionctc_updated_yearyears_of_experience_in_organizationdesignation_in_organizationclassstierlabel69989Others2020.0360000support engineer2020.03.03332160236otvqo ygraxzso wgqugqvnxgz2017.08000000data scientist2019.06.01111101242mvqwrvjo2001.03350000Others2019.022.01110136293nvnv wgzohrnvzwj otqcxwto2015.01220000fullstack engineer2021.08.0111127089wbt sqghu2011.01600000Others2019.012.02332# Output: (171316, 10)
<IPython.core.display.Javascript object># Output: <AxesSubplot:xlabel='orgyear', ylabel='ctc'>
# clusters.sample(5)
# clusters.shape
sns.scatterplot(clusters["orgyear"],               clusters["ctc"],               hue = clusters["label"])
based on above scatter plot , we can observe , a cluster of learners received CTC upto 30 LPA who joined after 2006-07.there's a group of learners who are very much experienced.and also learners joined after 2012-13 receiving CTC between 20 LPA to upto 1.5cr.# Output: 2000000.0
# Output: <AxesSubplot:xlabel='label'>
pd.crosstab(index = clusters["label"],    columns = clusters["tier"],values=clusters["ctc"],aggfunc= np.mean       ).plot(kind = "bar")
Based on k-Means Clustering algorithm output , as well as manual clustering , learners from tier1 company receiving very high CTC.# Output: <AxesSubplot:xlabel='label'>
pd.crosstab(index = clusters["label"],    columns = clusters["classs"],values=clusters["ctc"],aggfunc= np.mean       ).plot(kind = "bar")
# Output: <AxesSubplot:xlabel='label'>
pd.crosstab(index = clusters["label"],    columns = clusters["designation_in_organization"],            values=clusters["ctc"],aggfunc= np.mean       ).plot(kind = "bar")
# Output: <AxesSubplot:xlabel='years_of_experience_in_organization'>
pd.crosstab(columns = clusters["label"],    index = clusters["years_of_experience_in_organization"],            values=clusters["ctc"],aggfunc= np.mean       ).plot(kind = "bar")
# Cluster label 0 , are those learners who are very very experienced,experienced learners between 6 to 10 years of experience, earning above 40 LPA up tp 1.5Cr.In [266]:
Majority of Learners are experienced between 1 to 15 years . (49.73%)- (Cluster 2)there is a group of learners having 8 to upto 33 years of experience. (33%) - (Cluster 0)16.95% of learners who have experiences - (cluster 1)# Output: <AxesSubplot:xlabel='years_of_experience_in_organization'>
# Output: 2    49.7344091    33.3086230    16.956968Name: label, dtype: float64pd.crosstab(columns = clusters["label"],    index = clusters["years_of_experience_in_organization"],                   ).plot(kind = "bar")
clusters.label.value_counts(normalize=True)*100
years_of_experience_in_organization per each cluster group of learners
# Output: <AxesSubplot:xlabel='label'>
# Output: Index(['company_hash', 'orgyear', 'ctc', 'job_position', 'ctc_updated_year',      'years_of_experience_in_organization', 'designation_in_organization',      'classs', 'tier', 'label'],     dtype='object')  pd.crosstab(index = clusters["label"],    columns = clusters["tier"],            values=clusters["years_of_experience_in_organization"],            aggfunc=np.mean       ).plot(kind = "bar")
# clusters.columns
# Statistical Summury based on Each Cluster :In [288]:
# Output: label012ctccount2.905000e+045.706300e+048.520300e+04mean2.543348e+061.802940e+067.562107e+05std1.751976e+061.272597e+065.033019e+05min3.955000e+046.500000e+043.800000e+0425%1.420000e+061.000000e+064.000000e+0550%2.100000e+061.500000e+066.300000e+0575%3.147500e+062.200000e+061.000000e+06max1.250000e+071.250000e+075.600000e+06classscount2.905000e+045.706300e+048.520300e+04mean1.625886e+001.544574e+002.831191e+00std6.937293e-015.252113e-013.751798e-01min1.000000e+001.000000e+001.000000e+0025%1.000000e+001.000000e+003.000000e+0050%2.000000e+002.000000e+003.000000e+0075%2.000000e+002.000000e+003.000000e+00max3.000000e+003.000000e+003.000000e+00tiercount2.905000e+045.706300e+048.520300e+04mean1.484200e+001.648774e+002.900731e+00std6.478262e-015.742163e-013.010974e-01min1.000000e+001.000000e+001.000000e+0025%1.000000e+001.000000e+003.000000e+0050%1.000000e+002.000000e+003.000000e+0075%2.000000e+002.000000e+003.000000e+00max3.000000e+003.000000e+003.000000e+00years_of_experience_in_organizationcount2.905000e+045.706300e+048.520300e+04mean1.520678e+016.557945e+006.541436e+00std4.339403e+002.474935e+002.775220e+00min6.000000e+001.000000e+001.000000e+0025%1.200000e+015.000000e+004.000000e+0050%1.400000e+017.000000e+006.000000e+0075%1.700000e+018.000000e+008.000000e+00max3.300000e+011.300000e+011.700000e+01clusters.groupby("label").describe()[["ctc","classs","tier","years_of_experience_in_organization"]].T
